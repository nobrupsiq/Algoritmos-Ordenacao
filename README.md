
# Trabalho de implementa√ß√£o dos algoritmos de ordena√ß√£o




## Algoritmos üíª

- Selection Sort (ordena√ß√£o por sele√ß√£o)
- Bubble Sort
- Insertion Sort (ordena√ß√£o por inser√ß√£o)
- Merge Sort (ordena√ß√£o por intercala√ß√£o)
- Quick Sort (ordena√ß√£o r√°pida)
- Heap Sort
- Counting Sort
- Radix Sort
- Bucket Sort

## Aprendizados üìö


## 1. Selection Sort (Ordena√ß√£o por sele√ß√£o)
- O algoritmo percorre a lista v√°rias vezes, a cada passada selecionando o menor (ou maior) elemento e colocando-o na posi√ß√£o correta.

- Mostra a import√¢ncia de identificar m√≠nimos/m√°ximos e fazer trocas controladas.

- Utilizamos dois la√ßos for aninhados, onde um percorre a lista e o outro busca o menor elemento no restante da lista.

- O Selection Sort sempre faz o mesmo n√∫mero de compara√ß√µes, independente da entrada:
``O(n¬≤)`` no pior, m√©dio e melhor caso.
Isso √© √∫til para come√ßar a entender an√°lise de complexidade.

- ![App Screenshot](./.github/img/selectionSort.gif)

## 2. Bubble Sort

- O Bubble Sort percorre a lista v√°rias vezes, comparando elementos adjacentes e trocando-os de posi√ß√£o se estiverem fora de ordem.
- Isso nos ensina a detectar e corrigir posi√ß√µes erradas passo a passo.

- O nome "bubble" vem da ideia de que os maiores elementos "sobem" para o final da lista (como bolhas de ar na √°gua).

- Podemos introduzir uma otimiza√ß√£o que interrompe o algoritmo caso nenhuma troca seja feita em uma passada, mostrando a ideia de verifica√ß√£o de estado para melhorar desempenho.

- Mesmo com a otimiza√ß√£o, a complexidade no pior caso ainda √© ``O(n¬≤)``.

- Melhor caso (lista j√° ordenada): ``O(n)`` com a otimiza√ß√£o.

- Isso introduz a ideia de casos melhores e piores de desempenho.

- ![App Screenshot](./.github/img/bubble_sort.gif)

## 3. Insertion Sort

- A l√≥gica do Insertion Sort √© pegar um elemento e inseri-lo na posi√ß√£o correta dentro da parte da lista que j√° est√° ordenada.

- Assim como o Selection Sort, aprendemos a dividir a lista entre a parte ordenada e a n√£o ordenada, mas aqui a inser√ß√£o acontece de forma din√¢mica, deslocando elementos conforme necess√°rio.

- Ao inserir um novo elemento, outros precisam ser deslocados para a direita. Isso ensina como mover elementos dentro de uma estrutura como listas.

- Insertion Sort √© mais r√°pido que Selection e Bubble Sort quando a lista est√° quase ordenada, com desempenho ``O(n)`` no melhor caso.


- ![App Screenshot](./.github/img/insertion_sort.gif)

## 4. Merge Sort

- O Merge Sort aplica a estrat√©gia de "dividir para conquistar".

- Divide o problema em subproblemas menores (quebrando a lista ao meio), resolve os subproblemas (ordenando as metades), e combina (merge) os resultados em uma √∫nica lista ordenada.

- Merge Sort √© naturalmente recursivo.

- Junta duas listas ordenadas em uma s√≥.

- Merge Sort tem complexidade ``O(n log n)`` em todos os casos (melhor, m√©dio e pior).

- ![App Screenshot](./.github/img/merge_sort.gif)

## 5. Counting Sort

- Counting Sort √© um algoritmo n√£o-comparativo ‚Äî ele conta quantas vezes cada valor aparece, em vez de comparar elementos.

- Isso mostra que nem toda ordena√ß√£o precisa de ``if A > B`` para funcionar.

- O algoritmo s√≥ funciona bem quando sabemos o intervalo dos valores (por exemplo, de 0 a 100). Isso ensina que algoritmos podem ser super r√°pidos se usamos conhecimento extra sobre os dados. Exemplo: ordenar notas de alunos entre 0 e 10, ou idades entre 0 e 120.

- O Counting Sort usa um array auxiliar (vetor de contagem) para armazenar quantas vezes cada valor aparece.

- Quando os dados est√£o dentro de um intervalo pequeno, o Counting Sort tem complexidade ``O(n + k)``, onde:
  - ``n`` √© o n√∫mero de elementos,
  - ``k`` √© o valor m√°ximo (dom√≠nio dos dados).

- ![App Screenshot](./.github/img/counting_sort.gif)


## 6. Quick sort

- Assim como o Merge Sort, o Quick Sort divide o problema em partes menores

- Escolhe um ``piv√¥`` Coloca os elementos menores √† esquerda e maiores √† direita (particiona) e aplica o mesmo processo recursivamente

- O algoritmo usa recurs√£o para ordenar as sublistas criadas ap√≥s a parti√ß√£o.

- Com isso aprendemos a import√¢ncia da condi√ß√£o de parada e da chamada de fun√ß√£o sobre subconjuntos

- A etapa de particionar a lista em torno de um piv√¥ √© o cora√ß√£o do Quick Sort

- Nos ensina a controlar √≠ndices (ex: dois ponteiros), fazer compara√ß√µes e organizar os dados em tempo linear.

- A performance do Quick Sort depende da escolha do piv√¥

- Se for o pior poss√≠vel (ex: sempre o maior ou menor), o algoritmo vira ``O(n¬≤)``

- Com piv√¥s aleat√≥rios ou do meio, atinge ``O(n log n)`` na m√©dia.

- ![App Screenshot](./.github/img/quick_sort.gif)

## 7. Radix sort

- O Radix Sort ordena os elementos d√≠gito por d√≠gito, geralmente do menos significativo (LSD) para o mais significativo (MSD).

- sso ensina que √© poss√≠vel ordenar por "camadas" ou crit√©rios secund√°rios, um conceito √∫til para ordena√ß√µes complexas (ex: nomes por sobrenome, depois por nome).

- O Radix Sort depende de um algoritmo de ordena√ß√£o est√°vel para funcionar corretamente em cada d√≠gito (geralmente o Counting Sort).

- Funciona muito bem com n√∫meros inteiros dentro de um intervalo de comprimento razo√°vel. Dados com comprimento fixo (ex: n√∫meros com at√© 6 d√≠gitos, ou c√≥digos postais, CPF etc).

- Quando ``n`` √© o n√∫mero de elementos e ``k`` o n√∫mero de d√≠gitos, o Radix Sort tem complexidade ``O(n √ó k)``.

- Em muitos casos pr√°ticos, isso √© quase linear ``(O(n))``, tornando-o muito r√°pido.

- Isso refor√ßa a ideia de efici√™ncia al√©m da compara√ß√£o, diferente de algoritmos como Quick/Merge que t√™m limites te√≥ricos de ``O(n log n)`` com compara√ß√µes.

- Assim como Merge e Counting Sort, o Radix Sort usa mem√≥ria extra para listas auxiliares.

- Vantagens do Radix Sort

  - Quando os dados s√£o inteiros com um n√∫mero de d√≠gitos fixo e limitado, o Radix Sort pode ter desempenho quase linear ``(O(n))``, o que √© melhor que os ``O(n log n)`` dos algoritmos baseados em compara√ß√£o.

  - Ele ordena com base em posi√ß√µes/d√≠gitos, o que pode ser uma grande vantagem para tipos espec√≠ficos de dados ``(ex: CPFs, CEPs, IDs, etc.)``.

  - Como n√£o depende de compara√ß√µes, n√£o sofre com os piores casos do Quick Sort (por exemplo, quando a lista est√° quase ordenada).

- Desvantagens do Radix sort

  - S√≥ funciona bem com certos tipos de dados! Precisa que os dados sejam n√∫meros inteiros (ou convert√≠veis) e que o n√∫mero de d√≠gitos seja limitado.

  - Consome muita memoria! Usa estruturas auxiliares (como arrays/buckets) para cada d√≠gito, o que pode ser um problema em sistemas com mem√≥ria limitada.

  - Implementa√ß√£o mais complexa! √â mais dif√≠cil de implementar corretamente, especialmente com dados negativos, strings ou objetos personalizados.

  - Desempenho depende do comprimento dos dados! Se os n√∫meros tiverem muitos d√≠gitos (por exemplo, n√∫meros com 20 d√≠gitos), o algoritmo faz muitas passagens e pode ficar lento

- ![App Screenshot](./.github/img/radix_sort.gif)

## 8. Bucket sort

- Distribui√ß√£o em faixas (bucketiza√ß√£o)! O Bucket Sort distribui os elementos em baldes (subconjuntos) com base em seus valores (ex: de 0.0 a 0.1, de 0.1 a 0.2, etc.).

- Aprendemos a organizar dados em categorias/fatigamentos antes de ordenar ‚Äî √∫til para dados cont√≠nuos como n√∫meros decimais ou probabilidades.

- Combina√ß√£o de algoritmos! Cada "balde" pode ser ordenado com qualquer outro algoritmo (Insertion Sort √© comum).

- Isso mostra a ideia de hibridiza√ß√£o de algoritmos: usar o melhor algoritmo para cada parte do problema.

- Aloca√ß√£o din√¢mica de estruturas! Trabalha com listas de listas (ou arrays de arrays), o que refor√ßa habilidades com estrutura de dados din√¢micas.

- Efici√™ncia em dados uniformemente distribu√≠dos! Se os dados estiverem bem distribu√≠dos, o Bucket Sort pode chegar a tempo linear ``O(n)``.

- N√£o usa compara√ß√£o direta globalmente! A ordena√ß√£o global surge indiretamente pela ordena√ß√£o local de baldes ‚Äî outra forma de pensar fora do padr√£o "comparar elementos diretamente".

- Vantagens do Bucket sort
  - Muito r√°pido para dados bem distribu√≠dos! Quando os dados est√£o uniformemente distribu√≠dos (ex: entre 0 e 1), pode funcionar em tempo linear ``(O(n))``.

  - Boa base para combina√ß√µes! Pode ser combinado com outros algoritmos para otimizar desempenho, como usar Insertion Sort dentro dos baldes.

  - Simplicidade conceitual! A ideia de distribuir ‚Üí ordenar ‚Üí juntar √© f√°cil de entender conceitualmente.

  - Funciona bem com n√∫meros decimais! Diferente do Counting ou Radix, o Bucket Sort lida bem com valores fracion√°rios (floats).

- Desvantagens do Bucket sort

  - N√£o funciona bem com dados desbalanceados! Se os dados estiverem todos concentrados em poucos baldes, ele perde efici√™ncia (alguns baldes ficam vazios, outros cheios demais).

  - Escolher o n√∫mero e intervalo dos baldes √© dif√≠cil! Se n√£o for feito corretamente, o desempenho cai bastante. Isso exige conhecimento pr√©vio dos dados.

  - N√£o √© in-place! Cria listas auxiliares (os baldes), logo consome mais mem√≥ria.

  - N√£o √© ideal para dados com alta varia√ß√£o! Funciona melhor com dados cont√≠nuos e normalizados (ex: n√∫meros entre 0 e 1). Se os dados forem inteiros muito diferentes, outros algoritmos s√£o melhores.

  - Estabilidade depende da ordena√ß√£o interna! Por padr√£o, o Bucket Sort n√£o √© est√°vel, a menos que use um algoritmo interno est√°vel.

- ![App Screenshot](./.github/img/bucket_sort.gif)

# 9. Heap sort

- 